{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including pandas, numpy, seaborn, matplotlib, and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "# pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# numpy for numerical computations\n",
    "import numpy as np\n",
    "\n",
    "# seaborn for statistical data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn for machine learning and data processing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Inspect the Dataset\n",
    "Load the dataset using pandas and perform initial data inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Inspect the Dataset\n",
    "\n",
    "# Load the dataset using pandas\n",
    "df = pd.read_csv('stock_data.csv')\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Display the shape of the dataframe\n",
    "print('Shape of the dataframe:', df.shape)\n",
    "\n",
    "# Display the column names\n",
    "print('Columns in the dataframe:', df.columns)\n",
    "\n",
    "# Display the data types of each column\n",
    "print('Data types of the columns:')\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values in the dataframe\n",
    "print('Missing values in the dataframe:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display the summary statistics of the dataframe\n",
    "print('Summary statistics of the dataframe:')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Handle missing values, convert data types if necessary, and perform any other necessary preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# Handling missing values\n",
    "# If any column has more than 50% data missing, we drop the column\n",
    "half_count = len(df) / 2\n",
    "df = df.dropna(thresh=half_count, axis=1)\n",
    "\n",
    "# For the remaining missing values, we fill them with the mean of the respective column\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Convert data types if necessary\n",
    "# Here we ensure that all numerical columns are of float type\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numerical_cols] = df[numerical_cols].astype(float)\n",
    "\n",
    "# Check if there are any categorical variables. If yes, convert them to dummy variables\n",
    "categorical_cols = df.select_dtypes(include=[np.object]).columns\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Display the first 5 rows of the preprocessed dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Display the shape of the preprocessed dataframe\n",
    "print('Shape of the preprocessed dataframe:', df.shape)\n",
    "\n",
    "# Display the column names of the preprocessed dataframe\n",
    "print('Columns in the preprocessed dataframe:', df.columns)\n",
    "\n",
    "# Display the data types of each column in the preprocessed dataframe\n",
    "print('Data types of the columns in the preprocessed dataframe:')\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values in the preprocessed dataframe\n",
    "print('Missing values in the preprocessed dataframe:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display the summary statistics of the preprocessed dataframe\n",
    "print('Summary statistics of the preprocessed dataframe:')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Create new features that might be useful for the prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Create a new feature 'price_change' which is the difference between the closing price of the current day and the closing price of the previous day\n",
    "df['price_change'] = df['Close'].diff()\n",
    "\n",
    "# Create a new feature 'price_change_percentage' which is the percentage change in closing price from the previous day\n",
    "df['price_change_percentage'] = df['Close'].pct_change() * 100\n",
    "\n",
    "# Create a new feature 'price_volatility' which is the standard deviation of the closing price over the past 5 days\n",
    "df['price_volatility'] = df['Close'].rolling(window=5).std()\n",
    "\n",
    "# Create a new feature 'volume_change' which is the difference between the volume of the current day and the volume of the previous day\n",
    "df['volume_change'] = df['Volume'].diff()\n",
    "\n",
    "# Create a new feature 'volume_change_percentage' which is the percentage change in volume from the previous day\n",
    "df['volume_change_percentage'] = df['Volume'].pct_change() * 100\n",
    "\n",
    "# Create a new feature 'volume_volatility' which is the standard deviation of the volume over the past 5 days\n",
    "df['volume_volatility'] = df['Volume'].rolling(window=5).std()\n",
    "\n",
    "# Create a new feature 'high_low_spread' which is the difference between the high price and the low price of the day\n",
    "df['high_low_spread'] = df['High'] - df['Low']\n",
    "\n",
    "# Create a new feature 'close_open_spread' which is the difference between the closing price and the opening price of the day\n",
    "df['close_open_spread'] = df['Close'] - df['Open']\n",
    "\n",
    "# Drop the rows with missing values that were created due to feature engineering\n",
    "df = df.dropna()\n",
    "\n",
    "# Display the first 5 rows of the dataframe after feature engineering\n",
    "print(df.head())\n",
    "\n",
    "# Display the shape of the dataframe after feature engineering\n",
    "print('Shape of the dataframe after feature engineering:', df.shape)\n",
    "\n",
    "# Display the column names of the dataframe after feature engineering\n",
    "print('Columns in the dataframe after feature engineering:', df.columns)\n",
    "\n",
    "# Display the data types of each column in the dataframe after feature engineering\n",
    "print('Data types of the columns in the dataframe after feature engineering:')\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values in the dataframe after feature engineering\n",
    "print('Missing values in the dataframe after feature engineering:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display the summary statistics of the dataframe after feature engineering\n",
    "print('Summary statistics of the dataframe after feature engineering:')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Standardization\n",
    "Standardize the features to have zero mean and unit variance using sklearn's StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the StandardScaler from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Display the first 5 rows of the standardized dataframe\n",
    "print(df_scaled.head())\n",
    "\n",
    "# Display the shape of the standardized dataframe\n",
    "print('Shape of the standardized dataframe:', df_scaled.shape)\n",
    "\n",
    "# Display the column names of the standardized dataframe\n",
    "print('Columns in the standardized dataframe:', df_scaled.columns)\n",
    "\n",
    "# Display the data types of each column in the standardized dataframe\n",
    "print('Data types of the columns in the standardized dataframe:')\n",
    "print(df_scaled.dtypes)\n",
    "\n",
    "# Check for missing values in the standardized dataframe\n",
    "print('Missing values in the standardized dataframe:')\n",
    "print(df_scaled.isnull().sum())\n",
    "\n",
    "# Display the summary statistics of the standardized dataframe\n",
    "print('Summary statistics of the standardized dataframe:')\n",
    "print(df_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis\n",
    "Perform correlation analysis to identify which features are highly correlated with the 'high' or 'close' of the stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_scaled.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print('Correlation Matrix:')\n",
    "print(corr_matrix)\n",
    "\n",
    "# Plot the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Identify features that are highly correlated with 'High' and 'Close'\n",
    "high_corr = corr_matrix['High'].sort_values(ascending=False)\n",
    "close_corr = corr_matrix['Close'].sort_values(ascending=False)\n",
    "\n",
    "# Display the features highly correlated with 'High'\n",
    "print('Features highly correlated with High:')\n",
    "print(high_corr[high_corr > 0.5])\n",
    "\n",
    "# Display the features highly correlated with 'Close'\n",
    "print('Features highly correlated with Close:')\n",
    "print(close_corr[close_corr > 0.5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
